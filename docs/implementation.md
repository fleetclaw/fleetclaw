# FleetClaw Implementation Guide

How the v2 architecture maps to concrete OpenClaw, Redis, and Docker patterns.

This document complements `architecture.md` (the what) with the how. Read architecture.md first.

## SOUL.md and OpenClaw workspaces

OpenClaw agents read their identity from a workspace directory at `/home/node/.openclaw/workspace`. FleetClaw puts exactly two files there:

- **SOUL.md** — Generated by `generate-configs.py`. Contains the vanilla OpenClaw identity template with one injected line: `**You are {ASSET_ID}.** Serial: {SERIAL}.` For Clawvisor and Clawordinator, the identity line is just their name.
- **MEMORY.md** — Not generated. OpenClaw auto-creates it on the first agent session. The memory-curator skills define its structure and pruning rules.

The `skipBootstrap: true` setting in openclaw.json prevents OpenClaw from overwriting SOUL.md with its default template on first run.

## Skills and OpenClaw discovery

OpenClaw scans three locations for skills (in precedence order):

1. `<workspace>/skills/` — per-agent skills in the workspace
2. `<config_dir>/skills/` — skills alongside openclaw.json
3. Directories listed in `skills.load.extraDirs`

FleetClaw mounts skills as read-only Docker volumes at `/app/skills/{skill-name}` and declares `extraDirs: ["/app/skills"]` in openclaw.json. This tells OpenClaw to discover skills from our mount points.

Each skill directory contains a single `SKILL.md` file with:
- **YAML frontmatter** — `name`, `description`, `metadata.openclaw.requires` (bins, env)
- **Markdown body** — Trigger, Input, Behavior, Output, Overdue Condition (optional)

OpenClaw parses the frontmatter and stores custom metadata fields. The `requires.bins` field declares system packages the skill needs (installed in the Dockerfile, not by OpenClaw). The `requires.env` field declares environment variables.

### Skill mounting in Docker

Skills are mounted per agent type via docker-compose volumes:

```yaml
# Asset agent gets asset skills
- ./skills/fuel-logger:/app/skills/fuel-logger:ro
- ./skills/meter-reader:/app/skills/meter-reader:ro
- ./skills/pre-op:/app/skills/pre-op:ro
- ./skills/issue-reporter:/app/skills/issue-reporter:ro
- ./skills/nudger:/app/skills/nudger:ro
- ./skills/memory-curator-asset:/app/skills/memory-curator-asset:ro

# Clawvisor gets oversight skills
- ./skills/fleet-status:/app/skills/fleet-status:ro
- ./skills/compliance-tracker:/app/skills/compliance-tracker:ro
# ... etc
```

This is verbose but explicit. Each agent only sees the skills mounted to it.

## Heartbeats and OpenClaw config

OpenClaw's heartbeat system is configured in openclaw.json:

```json
"heartbeat": {
  "every": "30m",
  "prompt": "Run heartbeat tasks from your mounted skills.",
  "activeHours": {
    "start": "06:00",
    "end": "18:00",
    "timezone": "Australia/Perth"
  }
}
```

- `every` — Heartbeat interval. Asset agents: 30m, Clawvisor: 2h, Clawordinator: 4h.
- `prompt` — What the agent sees on each heartbeat tick. FleetClaw uses a generic prompt; skills define the actual heartbeat behavior.
- `activeHours` — Prevents heartbeats (and API costs) during off-hours. Values come from `fleet.yaml`.

### Cost implications

Each heartbeat is a full agent turn (~5K-15K input tokens). Budget at scale:

| Fleet size | Heartbeats/day | Notes |
|-----------|---------------|-------|
| 10 assets | ~500 | Manageable |
| 50 assets | ~2,500 | Moderate |
| 100 assets | ~4,800 | Consider 1h asset heartbeat |

`activeHours` is the primary cost control lever.

## Redis and skill commands

Skills interact with Redis via `redis-cli` (installed in the Dockerfile). The `REDIS_URL` environment variable is passed to every container.

### How skills describe Redis operations

Skills document Redis commands in their Output section as reference, not executable code:

```markdown
## Output
- **Redis writes:**
  XADD fleet:asset:{ASSET_ID}:fuel MAXLEN ~ 1000 * \
    liters {AMOUNT} \
    burn_rate {RATE} \
    source "operator"
```

The agent reads these instructions and executes the equivalent redis-cli commands via the exec tool. OpenClaw's exec tool is allowed by default (not in the `tools.deny` list).

### Consumer groups

Streams read by multiple independent consumers use consumer groups:

```bash
# Clawvisor and anomaly-detector both read fuel events
XGROUP CREATE fleet:asset:EX-001:fuel clawvisor $ MKSTREAM
XGROUP CREATE fleet:asset:EX-001:fuel anomaly-detector $ MKSTREAM
```

Consumer groups are created once by `setup-redis.sh` (generated by `generate-configs.py`). Each consumer tracks its own read position.

### Key patterns summary

| Pattern | Type | Purpose |
|---------|------|---------|
| `fleet:asset:{ID}:state` | HASH | Current machine state |
| `fleet:asset:{ID}:fuel` | STREAM | Fuel log events |
| `fleet:asset:{ID}:meter` | STREAM | Meter readings |
| `fleet:asset:{ID}:preop` | STREAM | Pre-op inspections |
| `fleet:asset:{ID}:issues` | STREAM | Issue reports |
| `fleet:asset:{ID}:maintenance` | STREAM | Maintenance events |
| `fleet:asset:{ID}:alerts` | STREAM | Anomaly alerts |
| `fleet:asset:{ID}:inbox` | STREAM | Messages to asset agent |
| `fleet:asset:{ID}:lifecycle` | HASH | Active/idle/decommissioned |
| `fleet:directives` | STREAM | Fleet-wide directives |
| `fleet:escalations` | STREAM | Escalation events |
| `fleet:index:active` | SET | Active asset IDs |
| `fleet:index:idle` | SET | Idle asset IDs |
| `fleet:index:type:{TYPE}` | SET | Assets by equipment type |

See `redis-schema.md` for full field definitions.

## Docker deployment

### Images

Two Dockerfiles, both extending the OpenClaw base image:

- `docker/Dockerfile` — Base + `redis-tools` + `jq`. Used by asset agents and Clawvisor.
- `docker/Dockerfile.clawordinator` — Base + `redis-tools` + `jq` + `docker.io`. Used by Clawordinator (Docker commands via TCP proxy, not direct socket).

### Container naming

- Asset agents: `fc-agent-{id}` (lowercased, hyphens removed)
- Clawvisor: `fc-clawvisor`
- Clawordinator: `fc-clawordinator`
- Docker socket proxy: `fc-docker-proxy`
- Redis: `fc-redis`

### Network

All containers share a single `fleetclaw` bridge network. Agents reach Redis via the `redis` service hostname.

### Volumes

```
./data/{ID}/                    → /home/node/.openclaw          (runtime state)
./output/workspaces/{ID}/       → /home/node/.openclaw/workspace (SOUL.md)
./output/config/openclaw-{ID}.json → /home/node/.openclaw/openclaw.json (config)
./skills/{name}/                → /app/skills/{name}            (skills, read-only)
```

Clawordinator accesses Docker via a TCP socket proxy (`tecnativa/docker-socket-proxy`) instead of a direct `/var/run/docker.sock` mount. This avoids giving the container root-equivalent access to the host. The proxy container exposes only the Docker API endpoints needed (containers, images, networks) on `tcp://fc-docker-proxy:2375`. Clawordinator connects via `DOCKER_HOST=tcp://fc-docker-proxy:2375`.

### Entrypoint

All agents use the explicit entrypoint:
```yaml
entrypoint: node
command: dist/index.js gateway
```

## Setup process

### Prerequisites

- Linux server with Docker and Docker Compose
- 16-32GB RAM (for ~64 agents + Redis)
- One Telegram bot token per agent (create via @BotFather)
- Fireworks API key (or other LLM provider)

### Five steps

```bash
# 1. Configure your fleet
cp fleet.yaml.example fleet.yaml
# Edit fleet.yaml — add your assets, contacts, timezone

# 2. Generate configs
python generate-configs.py
# Creates output/ with workspaces, configs, compose, env template, redis setup

# 3. Fill in credentials
cp output/.env.template .env
# Edit .env — add Telegram bot tokens, API key

# 4. Initialize Redis
bash output/setup-redis.sh
# Creates consumer groups for all asset streams

# 5. Launch
docker compose -f output/docker-compose.yml up -d
# Fleet is live. Agents create MEMORY.md on first operator interaction.
```

### Verification

After launch:
- `docker ps` — all containers should be healthy
- `docker logs fc-agent-ex001` — check for startup errors
- `redis-cli SMEMBERS fleet:index:active` — should list your asset IDs (after Clawordinator initializes)
- Message an asset's Telegram bot — agent should respond

## Key OpenClaw config options

| Option | Value | Why |
|--------|-------|-----|
| `skipBootstrap` | `true` | Don't overwrite generated SOUL.md |
| `bootstrapMaxChars` | `15000` | Leave headroom for skills context |
| `tools.deny` | `["browser","canvas","nodes","cron"]` | Fleet agents don't need these |
| `sandbox.mode` | `"off"` | No code execution sandboxing needed |
| `compaction.memoryFlush.softThresholdTokens` | `4000` | Trigger memory flush early |
| `skills.load.extraDirs` | `["/app/skills"]` | Tell OpenClaw where mounted skills are |

## Data flow recap

```
Operator texts machine bot
  → Asset agent processes via skills (fuel-logger, pre-op, etc.)
  → Writes to Redis streams + state HASH
  → Updates MEMORY.md via memory-curator

Clawvisor reads Redis on heartbeat (2h)
  → compliance-tracker checks timestamps
  → anomaly-detector scans for outliers
  → escalation-handler creates escalations
  → Writes alerts, escalations, inbox messages

Clawordinator reads Redis on heartbeat (4h)
  → escalation-resolver presents to leadership
  → fleet-director fans out directives
  → asset-lifecycle manages containers

Mechanic texts Clawvisor
  → maintenance-logger records repair
  → Writes to maintenance stream + asset inbox
  → Asset agent delivers acknowledgment to next operator
```

## Container resource limits

All containers have memory limits to prevent runaway processes from starving the host:

| Container | Memory limit | NODE_OPTIONS |
|-----------|-------------|-------------|
| Asset agents | 512m | `--max-old-space-size=384` |
| Clawvisor | 1g | `--max-old-space-size=384` |
| Clawordinator | 1g | `--max-old-space-size=384` |
| Redis | 768m | n/a |
| Docker proxy | 128m | n/a |

Log rotation is configured on all containers (`json-file` driver, `max-size: 10m`, `max-file: 3`) to prevent disk exhaustion.

Healthchecks run at 60-second intervals with a 60-second `start_period` to avoid check storms across large fleets.

## Host preparation

### Kernel tuning

OpenClaw containers use inotify for file watching. The Linux default `max_user_instances` (128) is too low for fleets with many containers.

```bash
# Required for fleets > ~30 assets
echo "fs.inotify.max_user_instances=8192" >> /etc/sysctl.conf
echo "fs.inotify.max_user_watches=524288" >> /etc/sysctl.conf
sysctl -p
```

Without this, containers may fail to start or skills may not be detected.

### Recommended host sizing

| Fleet size | RAM | CPU | Disk |
|-----------|-----|-----|------|
| 10 assets | 16 GB | 4 cores | 50 GB SSD |
| 50 assets | 32 GB | 8 cores | 100 GB SSD |
| 100 assets | 64 GB | 16 cores | 200 GB SSD |

Redis memory usage depends on stream depth. With `MAXLEN ~ 1000` per stream and 6 streams per asset, expect ~1-2 MB per asset under normal load.

## Operational considerations

### LLM provider rate limits

FleetClaw generates significant API traffic. Each heartbeat is a full agent turn (~5K-15K input tokens). At 30-minute intervals across a fleet:

- 10 assets: ~500 turns/day
- 50 assets: ~2,500 turns/day
- 100 assets: ~4,800 turns/day

Plus Clawvisor and Clawordinator heartbeats, plus operator-initiated conversations.

Check your Fireworks AI (or other provider) tier limits before deploying. Fleets over ~10 assets likely need at least a Tier 2 plan. Consider lengthening asset heartbeat to 1h for larger fleets.

### Secrets management

The `.env` file contains sensitive credentials (Telegram bot tokens, API keys). For production:

- Never commit `.env` to version control (already in `.gitignore`)
- Use `chmod 600 .env` to restrict file permissions
- Consider Docker secrets or a vault (HashiCorp Vault, AWS Secrets Manager) for Tier 2 deployments
- Rotate Fireworks API keys periodically
- Each Telegram bot token is unique per agent — a compromised token only affects one agent

### Redis consumer resilience

Consumer groups track read positions, but messages can get stuck in the Pending Entry List (PEL) if a consumer crashes mid-processing. For production fleets:

- Monitor PEL depth with `XPENDING fleet:asset:{ID}:fuel clawvisor`
- Use `XAUTOCLAIM` to reclaim messages stuck longer than a threshold (e.g., 30 minutes)
- Consider adding a periodic PEL cleanup to Clawvisor's heartbeat cycle

This is a Tier 2 concern — small fleets can rely on manual monitoring.

### Monitoring

Tier 1 deploys without dedicated monitoring infrastructure. For visibility:

- `docker ps` and `docker stats` for container health
- `docker logs fc-agent-{id}` for agent output
- `redis-cli INFO` for Redis memory and connection stats
- `redis-cli XLEN fleet:asset:{ID}:fuel` for stream depths

For Tier 2 deployments, consider:

- **cAdvisor** — container resource metrics (CPU, memory, network)
- **Prometheus + Grafana** — time-series metrics and dashboards
- **Redis Exporter** — Redis-specific metrics for Prometheus
- **Alerting** — container OOM kills, Redis memory > 80%, heartbeat failures

### Docker socket proxy security

The `tecnativa/docker-socket-proxy` restricts which Docker API endpoints Clawordinator can access. The generated compose enables:

- `CONTAINERS=1` — list, inspect, start, stop, restart containers
- `IMAGES=1` — list images (needed for status checks)
- `NETWORKS=1` — inspect network connectivity

POST actions (create, remove) are disabled by default. Clawordinator's `asset-onboarder` and `asset-lifecycle` skills work within these constraints by restarting/stopping existing containers. Full container creation (adding new assets at runtime) requires re-running `generate-configs.py` and `docker compose up`.
